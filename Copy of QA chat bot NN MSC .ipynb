{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18fJfNp7gYv2XlpbSYh_V6eejYzQCl5ll","timestamp":1721210376945}],"authorship_tag":"ABX9TyN5l4zYPTJPEOpR7sepH9ZU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install necessary libraries\n","!apt-get install -y tesseract-ocr\n","!apt-get install -y poppler-utils\n","!pip install pytesseract pillow sentence-transformers faiss-cpu transformers pdf2image pinecone-client"],"metadata":{"id":"7STa4-q9IT12"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_vCYoHpIRn2"},"outputs":[],"source":["import pytesseract  # OCR (Optical Character Recognition) tool to extract text from images\n","from PIL import Image  # Python Imaging Library to handle image files\n","from pdf2image import convert_from_path  # Convert PDF pages to images\n","from sentence_transformers import SentenceTransformer, util  # For sentence embeddings\n","from pinecone import Pinecone, ServerlessSpec  # Pinecone vector database for storing and querying embeddings\n","from tqdm.autonotebook import tqdm  # Progress bar for long loops\n","import numpy as np  # Numerical operations on arrays\n","from transformers import pipeline  # Huggingface's Transformers library for NLP tasks\n","import torch  # Deep learning library\n","import os  # Operating system interface\n","import pickle  # Serializing and deserializing Python objects\n","\n","# Step 1: Extract text from PDF using OCR\n","def extract_text_from_pdf(pdf_path):\n","    try:\n","        # Convert PDF pages to images\n","        images = convert_from_path(pdf_path)\n","        text = \"\"\n","        # Extract text from each image using pytesseract\n","        for img in images:\n","            text += pytesseract.image_to_string(img)\n","        return text\n","    except Exception as e:\n","        # Handle any exceptions that occur\n","        print(f\"An error occurred during PDF text extraction: {e}\")\n","        return \"\"\n","\n","# Step 2: Chunk the extracted text\n","def chunk_text(text, chunk_size=2000):\n","    # Split text into words\n","    words = text.split()\n","    chunks = []\n","    # Group words into chunks of specified size\n","    for i in range(0, len(words), chunk_size):\n","        chunks.append(' '.join(words[i:i + chunk_size]))\n","    return chunks\n","\n","# Step 3: Generate embeddings for the chunks in batches\n","def generate_embeddings(chunks, model, batch_size=10):\n","    try:\n","        embeddings = []\n","        # Generate embeddings in batches to handle large data\n","        for i in tqdm(range(0, len(chunks), batch_size)):\n","            batch_chunks = chunks[i:i + batch_size]\n","            # Encode text chunks to embeddings\n","            batch_embeddings = model.encode(batch_chunks, convert_to_tensor=True)\n","            embeddings.append(batch_embeddings.cpu().numpy())\n","        # Combine all batch embeddings into a single array\n","        embeddings = np.vstack(embeddings)\n","        return embeddings\n","    except Exception as e:\n","        # Handle any exceptions that occur\n","        print(f\"An error occurred during embedding generation: {e}\")\n","        return None\n","\n","# Step 4: Store embeddings in Pinecone\n","def store_embeddings_pinecone(index, embeddings, metadata):\n","    # Prepare vectors for upserting into Pinecone index\n","    vectors = [(str(i), embedding) for i, embedding in enumerate(embeddings)]\n","    # Upsert vectors into the Pinecone index\n","    index.upsert(vectors)\n","\n","# Step 5: Create the chatbot function for knowledge-based queries\n","def chatbot_query(query, model, index, chunk_map):\n","    try:\n","        # Encode the query to get its embedding\n","        query_embedding = model.encode([query], convert_to_tensor=True).cpu().numpy().tolist()\n","        # Query Pinecone index to find similar embeddings\n","        results = index.query(vector=query_embedding, top_k=1)\n","        # Get the best matching response from the chunk map\n","        best_response = chunk_map[int(results['matches'][0]['id'])]\n","        return best_response\n","    except Exception as e:\n","        # Handle any exceptions that occur\n","        print(f\"An error occurred during the chatbot query: {e}\")\n","        return \"Sorry, I couldn't find an answer.\"\n","\n","# Step 6: Create the document QA function with sequence handling\n","def qa_response_with_context_split(query, context, qa_pipeline, max_length=768):\n","    try:\n","        # Get the QA model's response using the context\n","        response = qa_pipeline(question=query, context=context)\n","        return response['answer']\n","    except Exception as e:\n","        # Handle any exceptions that occur\n","        print(f\"An error occurred during the document QA response: {e}\")\n","        return \"Sorry, I couldn't generate a response.\"\n","\n","# Step 7: Use NER to enhance the chatbotâ€™s understanding\n","def extract_entities(text, ner_pipeline):\n","    # Extract named entities from text using NER pipeline\n","    entities = ner_pipeline(text)\n","    return entities\n","\n","# Step 8: Implement feedback mechanism\n","def get_feedback():\n","    while True:\n","        # Ask user for feedback\n","        feedback = input(\"Was the response helpful? (yes/no): \").strip().lower()\n","        if feedback in ['yes', 'no']:\n","            return feedback\n","        # If invalid feedback, prompt user again\n","        print(\"Please answer with 'yes' or 'no'.\")\n","\n","# Updated interactive_chatbot function\n","def interactive_chatbot(knowledge_model, index, chunk_map, qa_pipeline, ner_pipeline):\n","    print(\"Welcome to the improved interactive chatbot! Ask your questions below. Type 'exit' to end the session.\")\n","    context = \"\"\n","    while True:\n","        # Get user query\n","        query = input(\"You: \")\n","        if query.lower() == 'exit':\n","            print(\"Goodbye!\")\n","            break\n","\n","        # Extract entities from the query\n","        entities = extract_entities(query, ner_pipeline)\n","        print(f\"Extracted Entities: {entities}\")\n","\n","        # First try to get a response from the knowledge-based system\n","        knowledge_response = chatbot_query(query, knowledge_model, index, chunk_map)\n","        print(f\"Knowledge-based Chatbot: {knowledge_response}\")\n","\n","        # Update context with the current query and response\n","        context += f\"\\nUser: {query}\\nBot: {knowledge_response}\"\n","\n","        # Get a response from the QA model with context splitting\n","        qa_chatbot_response = qa_response_with_context_split(query, context, qa_pipeline)\n","        print(f\"QA Chatbot: {qa_chatbot_response}\")\n","\n","        # Get feedback from the user\n","        feedback = get_feedback()\n","        print(f\"Feedback: {feedback}\")\n","        if feedback == \"no\":\n","            print(\"Sorry for the inconvenience. I'll try to improve.\")\n","\n","# Main function\n","def main(pdf_path, qa_model_name):\n","    # Extract text from PDF\n","    pdf_text = extract_text_from_pdf(pdf_path)\n","    print(f'There are {len(pdf_text)} characters in your document')\n","\n","    # Chunk the extracted text\n","    chunks = chunk_text(pdf_text)\n","\n","    # Print the chunks\n","    for i, chunk in enumerate(chunks):\n","        print(f\"Chunk {i}:\\n{chunk}\\n\")\n","\n","    # Initialize SentenceTransformer model for knowledge-based queries\n","    knowledge_model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n","\n","    # Generate embeddings for the chunks in batches and save to disk\n","    chunk_embeddings = generate_embeddings(chunks, knowledge_model)\n","\n","    if chunk_embeddings is None:\n","        print(\"Embedding generation failed. Exiting.\")\n","        return\n","\n","    # Save embeddings and chunks to disk\n","    with open('embeddings.pkl', 'wb') as f:\n","        pickle.dump(chunk_embeddings, f)\n","\n","    with open('chunk_map.pkl', 'wb') as f:\n","        pickle.dump({i: chunk for i, chunk in enumerate(chunks)}, f)\n","\n","    # Load embeddings and chunks from disk\n","    with open('embeddings.pkl', 'rb') as f:\n","        chunk_embeddings = pickle.load(f)\n","\n","    with open('chunk_map.pkl', 'rb') as f:\n","        chunk_map = pickle.load(f)\n","\n","    # Initialize Pinecone\n","    pc = Pinecone(api_key=\"8ea7eb99-cf4d-4ab3-a3a7-9d9057347011\")\n","    if 'chatbot' not in pc.list_indexes().names():\n","        pc.create_index(\n","            name=\"chatbot\",\n","            dimension=768,  # Replace with your model dimensions\n","            metric=\"cosine\",  # Replace with your model metric\n","            spec=ServerlessSpec(\n","                cloud=\"aws\",\n","                region=\"us-east-1\"\n","            )\n","        )\n","    index = pc.Index(\"chatbot\")\n","\n","    # Store embeddings in Pinecone\n","    store_embeddings_pinecone(index, chunk_embeddings, chunk_map)\n","\n","    # Load pre-trained QA model pipeline\n","    qa_pipeline = pipeline(\"question-answering\", model=qa_model_name)\n","\n","    # Load pre-trained NER model pipeline\n","    ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n","\n","    # Start the interactive chatbot session\n","    interactive_chatbot(knowledge_model, index, chunk_map, qa_pipeline, ner_pipeline)\n","\n","# Run the main function with the specified PDF path and model name\n","pdf_path = '/content/Pro Cv.pdf'\n","qa_model_name = 'deepset/roberta-large-squad2'  # Changed to a compatible QA model\n","main(pdf_path, qa_model_name)\n"]}]}